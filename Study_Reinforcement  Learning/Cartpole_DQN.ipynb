{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Cartpole_DQN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yEqXOFcO3415",
        "outputId": "04e57ec8-e42d-4930-80cc-d34c3ec0a94a"
      },
      "source": [
        "# install required system dependencies\n",
        "!apt-get install -y xvfb x11-utils\n",
        "\n",
        "# install required python dependencies (might need to install additional gym extras depending)\n",
        "!pip install gym[box2d]==0.17.* pyvirtualdisplay==0.2.* PyOpenGL==3.1.* PyOpenGL-accelerate==3.1.*"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "x11-utils is already the newest version (7.7+3build1).\n",
            "xvfb is already the newest version (2:1.19.6-1ubuntu4.9).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 37 not upgraded.\n",
            "Requirement already satisfied: gym[box2d]==0.17.* in /usr/local/lib/python3.7/dist-packages (0.17.3)\n",
            "Requirement already satisfied: pyvirtualdisplay==0.2.* in /usr/local/lib/python3.7/dist-packages (0.2.5)\n",
            "Requirement already satisfied: PyOpenGL==3.1.* in /usr/local/lib/python3.7/dist-packages (3.1.5)\n",
            "Requirement already satisfied: PyOpenGL-accelerate==3.1.* in /usr/local/lib/python3.7/dist-packages (3.1.5)\n",
            "Requirement already satisfied: EasyProcess in /usr/local/lib/python3.7/dist-packages (from pyvirtualdisplay==0.2.*) (0.3)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.7/dist-packages (from gym[box2d]==0.17.*) (1.19.5)\n",
            "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym[box2d]==0.17.*) (1.3.0)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym[box2d]==0.17.*) (1.5.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from gym[box2d]==0.17.*) (1.4.1)\n",
            "Requirement already satisfied: box2d-py~=2.3.5 in /usr/local/lib/python3.7/dist-packages (from gym[box2d]==0.17.*) (2.3.8)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym[box2d]==0.17.*) (0.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uSz0jQo-Kv7J"
      },
      "source": [
        "import pyvirtualdisplay\n",
        "\n",
        "\n",
        "_display = pyvirtualdisplay.Display(visible=False,  # use False with Xvfb\n",
        "                                    size=(1400, 900))\n",
        "_ = _display.start()"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eboc8vnK3zv2"
      },
      "source": [
        "import gym\n",
        "import math\n",
        "import random\n",
        "import numpy as np\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import namedtuple, deque\n",
        "from itertools import count\n",
        "from PIL import Image\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as T\n",
        "\n",
        "\n",
        "env = gym.make('CartPole-v0').unwrapped\n",
        "\n",
        "# matplotlib 설정\n",
        "is_ipython = 'inline' in matplotlib.get_backend()\n",
        "if is_ipython:\n",
        "    from IPython import display\n",
        "\n",
        "plt.ion()\n",
        "\n",
        "# GPU를 사용할 경우\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UJfHa2L137iS"
      },
      "source": [
        "Transition = namedtuple('Transition',\n",
        "                        ('state', 'action', 'next_state', 'reward'))\n",
        "\n",
        "\n",
        "class ReplayMemory(object):\n",
        "\n",
        "    def __init__(self, capacity):\n",
        "        self.memory = deque([],maxlen=capacity)\n",
        "\n",
        "    def push(self, *args):\n",
        "        \"\"\"transition 저장\"\"\"\n",
        "        self.memory.append(Transition(*args))\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        return random.sample(self.memory, batch_size)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.memory)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7d2yp_4v-lsj"
      },
      "source": [
        "class DQN(nn.Module):\n",
        "\n",
        "    def __init__(self, h, w, outputs):\n",
        "        super(DQN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 16, kernel_size=5, stride=2)\n",
        "        self.bn1 = nn.BatchNorm2d(16)\n",
        "        self.conv2 = nn.Conv2d(16, 32, kernel_size=5, stride=2)\n",
        "        self.bn2 = nn.BatchNorm2d(32)\n",
        "        self.conv3 = nn.Conv2d(32, 32, kernel_size=5, stride=2)\n",
        "        self.bn3 = nn.BatchNorm2d(32)\n",
        "\n",
        "        # Linear 입력의 연결 숫자는 conv2d 계층의 출력과 입력 이미지의 크기에\n",
        "        # 따라 결정되기 때문에 따로 계산을 해야합니다.\n",
        "        def conv2d_size_out(size, kernel_size = 5, stride = 2):\n",
        "            return (size - (kernel_size - 1) - 1) // stride  + 1\n",
        "        convw = conv2d_size_out(conv2d_size_out(conv2d_size_out(w)))\n",
        "        convh = conv2d_size_out(conv2d_size_out(conv2d_size_out(h)))\n",
        "        linear_input_size = convw * convh * 32\n",
        "        self.head = nn.Linear(linear_input_size, outputs)\n",
        "\n",
        "    # 최적화 중에 다음 행동을 결정하기 위해서 하나의 요소 또는 배치를 이용해 호촐됩니다.\n",
        "    # ([[left0exp,right0exp]...]) 를 반환합니다.\n",
        "    def forward(self, x):\n",
        "        x = x.to(device)\n",
        "        x = F.relu(self.bn1(self.conv1(x)))\n",
        "        x = F.relu(self.bn2(self.conv2(x)))\n",
        "        x = F.relu(self.bn3(self.conv3(x)))\n",
        "        return self.head(x.view(x.size(0), -1))"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 268
        },
        "id": "Z-W8ExAz_bfi",
        "outputId": "0dc2ef1c-c6a9-4f01-e30c-855d170aa610"
      },
      "source": [
        "resize = T.Compose([T.ToPILImage(),\n",
        "                    T.Resize(40, interpolation=Image.CUBIC),\n",
        "                    T.ToTensor()])\n",
        "\n",
        "\n",
        "def get_cart_location(screen_width):\n",
        "    world_width = env.x_threshold * 2\n",
        "    scale = screen_width / world_width\n",
        "    return int(env.state[0] * scale + screen_width / 2.0)  # MIDDLE OF CART\n",
        "\n",
        "def get_screen():\n",
        "    # gym이 요청한 화면은 400x600x3 이지만, 가끔 800x1200x3 처럼 큰 경우가 있습니다.\n",
        "    # 이것을 Torch order (CHW)로 변환한다.\n",
        "    screen = env.render(mode='rgb_array').transpose((2, 0, 1))\n",
        "    # 카트는 아래쪽에 있으므로 화면의 상단과 하단을 제거하십시오.\n",
        "    _, screen_height, screen_width = screen.shape\n",
        "    screen = screen[:, int(screen_height*0.4):int(screen_height * 0.8)]\n",
        "    view_width = int(screen_width * 0.6)\n",
        "    cart_location = get_cart_location(screen_width)\n",
        "    if cart_location < view_width // 2:\n",
        "        slice_range = slice(view_width)\n",
        "    elif cart_location > (screen_width - view_width // 2):\n",
        "        slice_range = slice(-view_width, None)\n",
        "    else:\n",
        "        slice_range = slice(cart_location - view_width // 2,\n",
        "                            cart_location + view_width // 2)\n",
        "    # 카트를 중심으로 정사각형 이미지가 되도록 가장자리를 제거하십시오.\n",
        "    screen = screen[:, :, slice_range]\n",
        "    # float 으로 변환하고,  rescale 하고, torch tensor 로 변환하십시오.\n",
        "    # (이것은 복사를 필요로하지 않습니다)\n",
        "    screen = np.ascontiguousarray(screen, dtype=np.float32) / 255\n",
        "    screen = torch.from_numpy(screen)\n",
        "    # 크기를 수정하고 배치 차원(BCHW)을 추가하십시오.\n",
        "    return resize(screen).unsqueeze(0)\n",
        "\n",
        "\n",
        "env.reset()\n",
        "plt.figure()\n",
        "plt.imshow(get_screen().cpu().squeeze(0).permute(1, 2, 0).numpy(),\n",
        "           interpolation='none')\n",
        "plt.title('Example extracted screen')\n",
        "plt.show()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/transforms.py:288: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2GQ97fC3Avlv"
      },
      "source": [
        "BATCH_SIZE = 128\n",
        "GAMMA = 0.999\n",
        "EPS_START = 0.9\n",
        "EPS_END = 0.05\n",
        "EPS_DECAY = 200\n",
        "TARGET_UPDATE = 10\n",
        "\n",
        "# AI gym에서 반환된 형태를 기반으로 계층을 초기화 하도록 화면의 크기를\n",
        "# 가져옵니다. 이 시점에 일반적으로 3x40x90 에 가깝습니다.\n",
        "# 이 크기는 get_screen()에서 고정, 축소된 렌더 버퍼의 결과입니다.\n",
        "init_screen = get_screen()\n",
        "_, _, screen_height, screen_width = init_screen.shape\n",
        "\n",
        "# gym 행동 공간에서 행동의 숫자를 얻습니다.\n",
        "n_actions = env.action_space.n\n",
        "\n",
        "policy_net = DQN(screen_height, screen_width, n_actions).to(device)\n",
        "target_net = DQN(screen_height, screen_width, n_actions).to(device)\n",
        "target_net.load_state_dict(policy_net.state_dict())\n",
        "target_net.eval()\n",
        "\n",
        "optimizer = optim.RMSprop(policy_net.parameters())\n",
        "memory = ReplayMemory(10000)\n",
        "\n",
        "\n",
        "steps_done = 0\n",
        "\n",
        "\n",
        "def select_action(state):\n",
        "    global steps_done\n",
        "    sample = random.random()\n",
        "    eps_threshold = EPS_END + (EPS_START - EPS_END) * \\\n",
        "        math.exp(-1. * steps_done / EPS_DECAY)\n",
        "    steps_done += 1\n",
        "    if sample > eps_threshold:\n",
        "        with torch.no_grad():\n",
        "            # t.max (1)은 각 행의 가장 큰 열 값을 반환합니다.\n",
        "            # 최대 결과의 두번째 열은 최대 요소의 주소값이므로,\n",
        "            # 기대 보상이 더 큰 행동을 선택할 수 있습니다.\n",
        "            return policy_net(state).max(1)[1].view(1, 1)\n",
        "    else:\n",
        "        return torch.tensor([[random.randrange(n_actions)]], device=device, dtype=torch.long)\n",
        "\n",
        "\n",
        "episode_durations = []\n",
        "\n",
        "\n",
        "def plot_durations():\n",
        "    plt.figure(2)\n",
        "    plt.clf()\n",
        "    durations_t = torch.tensor(episode_durations, dtype=torch.float)\n",
        "    plt.title('Training...')\n",
        "    plt.xlabel('Episode')\n",
        "    plt.ylabel('Duration')\n",
        "    plt.plot(durations_t.numpy())\n",
        "    # 100개의 에피소드 평균을 가져 와서 도표 그리기\n",
        "    if len(durations_t) >= 100:\n",
        "        means = durations_t.unfold(0, 100, 1).mean(1).view(-1)\n",
        "        means = torch.cat((torch.zeros(99), means))\n",
        "        plt.plot(means.numpy())\n",
        "\n",
        "    plt.pause(0.001)  # 도표가 업데이트되도록 잠시 멈춤\n",
        "    if is_ipython:\n",
        "        display.clear_output(wait=True)\n",
        "        display.display(plt.gcf())"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tQLLSf1JKyGj"
      },
      "source": [
        "def optimize_model():\n",
        "  if len(memory) < BATCH_SIZE:\n",
        "    return\n",
        "  \n",
        "  transistion = memory.sample(BATCH_SIZE)\n",
        "  batch = Transition(*zip(*transistion))\n",
        "\n",
        "  non_final_mask = torch.tensor(tuple(map(lambda s: s is not None, batch.next_state)))"
      ],
      "execution_count": 6,
      "outputs": []
    }
  ]
}